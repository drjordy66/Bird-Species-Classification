{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Competition - Milestone 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import urllib.request\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "x = pickle.load(urllib.request.urlopen('https://s3.amazonaws.com/stat558drjordankaggle/train_features'))\n",
    "test_set = pickle.load(urllib.request.urlopen('https://s3.amazonaws.com/stat558drjordankaggle/test_features'))\n",
    "y = np.array(pickle.load(urllib.request.urlopen('https://s3.amazonaws.com/stat558drjordankaggle/train_labels')))\n",
    "y = y.astype(float)\n",
    "\n",
    "# image transformations not used for this milestone\n",
    "# color_set = pickle.load(urllib.request.urlopen('https://s3.amazonaws.com/stat558drjordankaggle/color_features'))\n",
    "# compress_set = pickle.load(urllib.request.urlopen('https://s3.amazonaws.com/stat558drjordankaggle/compress_features'))\n",
    "# crop_set = pickle.load(urllib.request.urlopen('https://s3.amazonaws.com/stat558drjordankaggle/crop_features'))\n",
    "# crop_to_corner_set = pickle.load(urllib.request.urlopen('https://s3.amazonaws.com/stat558drjordankaggle/crop_to_corner_features'))\n",
    "# homography_set = pickle.load(urllib.request.urlopen('https://s3.amazonaws.com/stat558drjordankaggle/homography_features'))\n",
    "# mirror_set = pickle.load(urllib.request.urlopen('https://s3.amazonaws.com/stat558drjordankaggle/mirror_features'))\n",
    "# rotate30_set = pickle.load(urllib.request.urlopen('https://s3.amazonaws.com/stat558drjordankaggle/rotate30_features'))\n",
    "# scale_set = pickle.load(urllib.request.urlopen('https://s3.amazonaws.com/stat558drjordankaggle/scale_features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeobj(beta, lamb, x, y):\n",
    "    # number of observations\n",
    "    n = x.shape[0]\n",
    "    \n",
    "    # compute objective function\n",
    "    obj = (1/n)*(np.sum(np.log(1 + np.exp(-y*np.dot(x, beta))))) + lamb*np.sum(beta**2)\n",
    "    \n",
    "    return obj\n",
    "\n",
    "\n",
    "def computegrad(beta, lamb, x, y):\n",
    "    # number of observations\n",
    "    n = x.shape[0]\n",
    "    \n",
    "    # compute gradient of objective function\n",
    "    grad_beta = -(1/n)*(np.dot(x.T, y/(np.exp(y*np.dot(x, beta)) + 1))) + 2*lamb*beta\n",
    "    \n",
    "    return grad_beta\n",
    "\n",
    "\n",
    "def backtracking(beta, lamb, x, y, eta=1, alpha=0.5, gamma=0.8, max_iter=100):\n",
    "    # initialize variables\n",
    "    grad_beta = computegrad(beta, lamb, x, y)\n",
    "    norm_grad_beta = np.sqrt(np.sum(grad_beta**2))\n",
    "    found_eta = 0\n",
    "    t = 0\n",
    "    \n",
    "    # loop through until eta found or max iterations reached\n",
    "    while found_eta == 0 and t < max_iter:\n",
    "        if (computeobj(beta - eta*grad_beta, lamb, x, y) <\n",
    "                computeobj(beta, lamb, x, y) - alpha*eta*norm_grad_beta**2):\n",
    "            found_eta = 1\n",
    "        elif t == max_iter:\n",
    "            break\n",
    "        else:\n",
    "            eta = eta*gamma\n",
    "            t += 1\n",
    "    \n",
    "    return eta\n",
    "\n",
    "\n",
    "def fastgradalgo(beta_init, theta_init, lamb, x, y, max_iter, eps):\n",
    "    # initialize variables\n",
    "    beta = beta_init\n",
    "    theta = theta_init\n",
    "    grad_theta = computegrad(theta, lamb, x, y)\n",
    "    eta_init = 1/(max(np.linalg.eigh(np.dot((1/n)*x.T, x))[0]) + lamb)\n",
    "    beta_vals = [beta_init]\n",
    "    t = 0\n",
    "    \n",
    "    # loop through until EITHER max iterations reached or threshold of epsilon reached\n",
    "    while t < max_iter and np.linalg.norm(grad_theta) >  eps:\n",
    "        eta = backtracking(beta, lamb, x, y, eta=eta_init)\n",
    "        beta_next = theta - eta*grad_theta\n",
    "        theta = beta_next + t*(beta_next - beta)/(t + 3)\n",
    "        grad_theta = computegrad(theta, lamb, x, y)\n",
    "        beta = beta_next\n",
    "        beta_vals.append(beta)\n",
    "        t += 1\n",
    "        \n",
    "    return beta_vals\n",
    "\n",
    "\n",
    "def split_data_equal(x, y, test_set, rand_state=5, train_size=0.75):\n",
    "    # split into train and test sets\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=train_size, random_state=rand_state, stratify=y)\n",
    "    \n",
    "    # center and standardize x values\n",
    "    x_scaler = StandardScaler().fit(x_train)\n",
    "    x_train = x_scaler.transform(x_train)\n",
    "    x_test = x_scaler.transform(x_test)\n",
    "    test_set = x_scaler.transform(test_set)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test, test_set\n",
    "\n",
    "\n",
    "def train_alg(x, y, classes, lamb_list):\n",
    "    # initialize betas and create list for final beta values\n",
    "    beta_init = np.zeros(d)\n",
    "    theta_init = np.zeros(d)\n",
    "    final_betas = []\n",
    "    y_values = []\n",
    "\n",
    "    # loop through each label and perform ovr appending the final betas for each class\n",
    "    for i in range(len(classes)):\n",
    "        lamb = lamb_list[i]\n",
    "        y_binary = copy.deepcopy(y)\n",
    "        y_binary[y != classes[i]] = -1\n",
    "        y_binary[y == classes[i]] = 1\n",
    "        betas = fastgradalgo(beta_init=beta_init, theta_init=theta_init, lamb=lamb, x=x_train, y=y_binary, max_iter=1000, eps=1e-5)[-1]\n",
    "        final_betas.append(betas)\n",
    "        \n",
    "    return np.array(final_betas)\n",
    "\n",
    "\n",
    "def predict(x, betas, classes):\n",
    "    # initialize calculated y values\n",
    "    y_values = []\n",
    "    \n",
    "    # loop through set of final betas and calculate y values\n",
    "    for i in range(len(betas)):\n",
    "        y_values.append(np.dot(x, betas[i]))\n",
    "    \n",
    "    # calculate predicted values\n",
    "    y_predict = classes[np.argmax(np.array(y_values), axis=0)]\n",
    "    \n",
    "    return y_predict\n",
    "\n",
    "\n",
    "def accuracy_misclass_error(predict, actual):\n",
    "    # calculate misclassification error\n",
    "    misclass_error = np.mean(predict != actual)*100\n",
    "    accuracy = 100 - misclass_error\n",
    "    \n",
    "    return accuracy, misclass_error\n",
    "\n",
    "\n",
    "def display_confusion_matrix(predict, actual):\n",
    "    # calculate confusion matrix\n",
    "    conf_mat = confusion_matrix(y_true=actual, y_pred=predict)\n",
    "    \n",
    "    # build visual plot\n",
    "    plt.matshow(conf_mat);\n",
    "    plt.title('Confusion Matrix');\n",
    "    plt.xlabel('Predicted Label');\n",
    "    plt.ylabel('True Label');\n",
    "    plt.xticks(range(len(classes)), classes);\n",
    "    plt.yticks(range(len(classes)), classes);\n",
    "    \n",
    "    fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "    fig_size[0] = 18\n",
    "    fig_size[1] = 12\n",
    "    plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "    \n",
    "    return conf_mat\n",
    "    \n",
    "\n",
    "def class_diff(classes, conf_matrix):\n",
    "    # initialize variable to append each individual class percent\n",
    "    percent_correct = []\n",
    "    \n",
    "    # loop through confusion matrix by true label\n",
    "    for i in range(len(conf_matrix[0, :])):\n",
    "        class_count = np.sum(conf_matrix[i])\n",
    "        misclass_count = 0\n",
    "        \n",
    "        # loop through confusion matrix by predict label and append percent correct\n",
    "        for j in range(len(conf_matrix[:, 0])):\n",
    "            if i != j:\n",
    "                misclass_count += conf_matrix[i][j]\n",
    "            else:\n",
    "                pass\n",
    "        percent_correct.append(misclass_count/class_count)\n",
    "        \n",
    "    # calcuate ordered list of multi-class misclassification error\n",
    "    ordered_class_diff = np.vstack((classes, np.array(percent_correct))).T\n",
    "    ordered_class_diff = ordered_class_diff[ordered_class_diff[:, 1].argsort()[::-1]]\n",
    "    \n",
    "    return ordered_class_diff\n",
    "    \n",
    "\n",
    "def decomp_PCA(train, test, test_set, explained_var_threshold=0.95):\n",
    "    pca = PCA().fit(train)\n",
    "\n",
    "    pca_explained_var_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "    pca_explained_var = []\n",
    "    num_component_vectors = 0\n",
    "\n",
    "    while np.sum(pca_explained_var) < explained_var_threshold:\n",
    "        pca_explained_var.append(pca_explained_var_ratio[num_component_vectors])\n",
    "        num_component_vectors += 1\n",
    "    #print('# Component Vectors: %d    Explained Var: %f' % (num_component_vectors, np.sum(pca_explained_var)))\n",
    "\n",
    "    pca = PCA(n_components=num_component_vectors).fit(train)\n",
    "    x_train = pca.transform(train)\n",
    "    x_test = pca.transform(test)\n",
    "    test_set = pca.transform(test_set)\n",
    "    \n",
    "    return x_train, x_test, test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pick $k = 5$ classes of your choice from the dataset. You may choose any subset of 5 classes among all classes of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# subset the data for the first five classes\n",
    "classes = np.unique(y)[0:5]\n",
    "index = np.ravel(np.nonzero(np.in1d(y, classes)))\n",
    "x_subset = x[index]\n",
    "y_subset = y[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write a function that, for any class at hand, creates a training set with an equal number of examples from the class at hand and from the other classes. You may simply randomly pick the examples from the other classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# see above \"split_data_equal\" function\n",
    "\n",
    "# split data\n",
    "x_train, x_test, y_train, y_test, test_set = split_data_equal(x_subset, y_subset, test_set)\n",
    "n = x_train.shape[0]\n",
    "d = x_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For each class _c_, train an $\\ell_2^2$-regularized logistic regression classiﬁer using _your own fast gradient algorithm_ with $\\lambda_c=1$. Display the confusion matrix. Which classes seem to be the most diﬃcult to classify?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "betas = train_alg(x=x_train, y=y_train, classes=classes, lamb_list=[1, 1, 1, 1, 1])\n",
    "y_predict = predict(x=x_test, betas=betas, classes=classes)\n",
    "accuracy, misclass_error = accuracy_misclass_error(predict=y_predict, actual=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare coefficients from my own _fast gradient algorithm_ to those from scikit learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2038</th>\n",
       "      <th>2039</th>\n",
       "      <th>2040</th>\n",
       "      <th>2041</th>\n",
       "      <th>2042</th>\n",
       "      <th>2043</th>\n",
       "      <th>2044</th>\n",
       "      <th>2045</th>\n",
       "      <th>2046</th>\n",
       "      <th>2047</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.000394</td>\n",
       "      <td>-0.000520</td>\n",
       "      <td>0.000369</td>\n",
       "      <td>-0.008736</td>\n",
       "      <td>0.001789</td>\n",
       "      <td>-0.003680</td>\n",
       "      <td>-0.004604</td>\n",
       "      <td>-0.003171</td>\n",
       "      <td>-0.005278</td>\n",
       "      <td>-0.002190</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>-0.007627</td>\n",
       "      <td>0.007183</td>\n",
       "      <td>0.000971</td>\n",
       "      <td>0.013051</td>\n",
       "      <td>0.002880</td>\n",
       "      <td>0.008109</td>\n",
       "      <td>-0.009810</td>\n",
       "      <td>0.004303</td>\n",
       "      <td>-0.001887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000586</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.004743</td>\n",
       "      <td>0.012820</td>\n",
       "      <td>-0.004775</td>\n",
       "      <td>-0.000732</td>\n",
       "      <td>0.016167</td>\n",
       "      <td>0.010797</td>\n",
       "      <td>0.002612</td>\n",
       "      <td>-0.003186</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002564</td>\n",
       "      <td>0.011356</td>\n",
       "      <td>-0.005319</td>\n",
       "      <td>-0.003187</td>\n",
       "      <td>-0.006499</td>\n",
       "      <td>-0.009484</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.015603</td>\n",
       "      <td>-0.003333</td>\n",
       "      <td>-0.003804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.011003</td>\n",
       "      <td>0.002882</td>\n",
       "      <td>-0.011399</td>\n",
       "      <td>0.001417</td>\n",
       "      <td>-0.000084</td>\n",
       "      <td>-0.000513</td>\n",
       "      <td>0.003430</td>\n",
       "      <td>-0.002197</td>\n",
       "      <td>-0.004508</td>\n",
       "      <td>0.011429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002081</td>\n",
       "      <td>-0.005939</td>\n",
       "      <td>-0.005204</td>\n",
       "      <td>0.005689</td>\n",
       "      <td>0.002806</td>\n",
       "      <td>0.000785</td>\n",
       "      <td>-0.006149</td>\n",
       "      <td>0.008245</td>\n",
       "      <td>0.014035</td>\n",
       "      <td>0.003263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.014864</td>\n",
       "      <td>-0.006269</td>\n",
       "      <td>0.018245</td>\n",
       "      <td>-0.001961</td>\n",
       "      <td>0.000986</td>\n",
       "      <td>0.000688</td>\n",
       "      <td>-0.002638</td>\n",
       "      <td>-0.001666</td>\n",
       "      <td>-0.006747</td>\n",
       "      <td>-0.008210</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002895</td>\n",
       "      <td>0.010624</td>\n",
       "      <td>0.005879</td>\n",
       "      <td>-0.013033</td>\n",
       "      <td>0.004165</td>\n",
       "      <td>-0.000437</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>-0.001383</td>\n",
       "      <td>-0.005296</td>\n",
       "      <td>0.003090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001524</td>\n",
       "      <td>-0.002425</td>\n",
       "      <td>-0.008089</td>\n",
       "      <td>0.001256</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>0.003146</td>\n",
       "      <td>-0.009862</td>\n",
       "      <td>-0.007076</td>\n",
       "      <td>0.011734</td>\n",
       "      <td>-0.001042</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008517</td>\n",
       "      <td>-0.002372</td>\n",
       "      <td>-0.001015</td>\n",
       "      <td>0.010934</td>\n",
       "      <td>-0.007737</td>\n",
       "      <td>0.009299</td>\n",
       "      <td>-0.005106</td>\n",
       "      <td>-0.005191</td>\n",
       "      <td>-0.008181</td>\n",
       "      <td>-0.004550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2048 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1         2         3         4         5         6     \\\n",
       "0 -0.000394 -0.000520  0.000369 -0.008736  0.001789 -0.003680 -0.004604   \n",
       "1  0.000586  0.001529  0.004743  0.012820 -0.004775 -0.000732  0.016167   \n",
       "2  0.011003  0.002882 -0.011399  0.001417 -0.000084 -0.000513  0.003430   \n",
       "3 -0.014864 -0.006269  0.018245 -0.001961  0.000986  0.000688 -0.002638   \n",
       "4  0.001524 -0.002425 -0.008089  0.001256  0.000303  0.003146 -0.009862   \n",
       "\n",
       "       7         8         9       ...         2038      2039      2040  \\\n",
       "0 -0.003171 -0.005278 -0.002190    ...     0.000512 -0.007627  0.007183   \n",
       "1  0.010797  0.002612 -0.003186    ...    -0.002564  0.011356 -0.005319   \n",
       "2 -0.002197 -0.004508  0.011429    ...     0.002081 -0.005939 -0.005204   \n",
       "3 -0.001666 -0.006747 -0.008210    ...    -0.002895  0.010624  0.005879   \n",
       "4 -0.007076  0.011734 -0.001042    ...     0.008517 -0.002372 -0.001015   \n",
       "\n",
       "       2041      2042      2043      2044      2045      2046      2047  \n",
       "0  0.000971  0.013051  0.002880  0.008109 -0.009810  0.004303 -0.001887  \n",
       "1 -0.003187 -0.006499 -0.009484  0.000241  0.015603 -0.003333 -0.003804  \n",
       "2  0.005689  0.002806  0.000785 -0.006149  0.008245  0.014035  0.003263  \n",
       "3 -0.013033  0.004165 -0.000437  0.000303 -0.001383 -0.005296  0.003090  \n",
       "4  0.010934 -0.007737  0.009299 -0.005106 -0.005191 -0.008181 -0.004550  \n",
       "\n",
       "[5 rows x 2048 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2038</th>\n",
       "      <th>2039</th>\n",
       "      <th>2040</th>\n",
       "      <th>2041</th>\n",
       "      <th>2042</th>\n",
       "      <th>2043</th>\n",
       "      <th>2044</th>\n",
       "      <th>2045</th>\n",
       "      <th>2046</th>\n",
       "      <th>2047</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.000394</td>\n",
       "      <td>-0.000520</td>\n",
       "      <td>0.000369</td>\n",
       "      <td>-0.008736</td>\n",
       "      <td>0.001789</td>\n",
       "      <td>-0.003679</td>\n",
       "      <td>-0.004604</td>\n",
       "      <td>-0.003171</td>\n",
       "      <td>-0.005278</td>\n",
       "      <td>-0.002190</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>-0.007628</td>\n",
       "      <td>0.007182</td>\n",
       "      <td>0.000971</td>\n",
       "      <td>0.013050</td>\n",
       "      <td>0.002880</td>\n",
       "      <td>0.008109</td>\n",
       "      <td>-0.009810</td>\n",
       "      <td>0.004303</td>\n",
       "      <td>-0.001886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000586</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.004743</td>\n",
       "      <td>0.012820</td>\n",
       "      <td>-0.004775</td>\n",
       "      <td>-0.000732</td>\n",
       "      <td>0.016167</td>\n",
       "      <td>0.010797</td>\n",
       "      <td>0.002612</td>\n",
       "      <td>-0.003186</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002564</td>\n",
       "      <td>0.011355</td>\n",
       "      <td>-0.005319</td>\n",
       "      <td>-0.003187</td>\n",
       "      <td>-0.006499</td>\n",
       "      <td>-0.009484</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.015603</td>\n",
       "      <td>-0.003334</td>\n",
       "      <td>-0.003804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.011003</td>\n",
       "      <td>0.002882</td>\n",
       "      <td>-0.011400</td>\n",
       "      <td>0.001417</td>\n",
       "      <td>-0.000084</td>\n",
       "      <td>-0.000513</td>\n",
       "      <td>0.003430</td>\n",
       "      <td>-0.002197</td>\n",
       "      <td>-0.004508</td>\n",
       "      <td>0.011429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002081</td>\n",
       "      <td>-0.005939</td>\n",
       "      <td>-0.005204</td>\n",
       "      <td>0.005689</td>\n",
       "      <td>0.002806</td>\n",
       "      <td>0.000785</td>\n",
       "      <td>-0.006149</td>\n",
       "      <td>0.008245</td>\n",
       "      <td>0.014035</td>\n",
       "      <td>0.003263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.014864</td>\n",
       "      <td>-0.006269</td>\n",
       "      <td>0.018245</td>\n",
       "      <td>-0.001961</td>\n",
       "      <td>0.000986</td>\n",
       "      <td>0.000688</td>\n",
       "      <td>-0.002638</td>\n",
       "      <td>-0.001666</td>\n",
       "      <td>-0.006747</td>\n",
       "      <td>-0.008210</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002895</td>\n",
       "      <td>0.010624</td>\n",
       "      <td>0.005879</td>\n",
       "      <td>-0.013033</td>\n",
       "      <td>0.004165</td>\n",
       "      <td>-0.000437</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>-0.001383</td>\n",
       "      <td>-0.005297</td>\n",
       "      <td>0.003090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001524</td>\n",
       "      <td>-0.002425</td>\n",
       "      <td>-0.008089</td>\n",
       "      <td>0.001256</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>0.003146</td>\n",
       "      <td>-0.009862</td>\n",
       "      <td>-0.007076</td>\n",
       "      <td>0.011734</td>\n",
       "      <td>-0.001042</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008517</td>\n",
       "      <td>-0.002372</td>\n",
       "      <td>-0.001015</td>\n",
       "      <td>0.010934</td>\n",
       "      <td>-0.007737</td>\n",
       "      <td>0.009299</td>\n",
       "      <td>-0.005106</td>\n",
       "      <td>-0.005191</td>\n",
       "      <td>-0.008182</td>\n",
       "      <td>-0.004550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2048 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1         2         3         4         5         6     \\\n",
       "0 -0.000394 -0.000520  0.000369 -0.008736  0.001789 -0.003679 -0.004604   \n",
       "1  0.000586  0.001529  0.004743  0.012820 -0.004775 -0.000732  0.016167   \n",
       "2  0.011003  0.002882 -0.011400  0.001417 -0.000084 -0.000513  0.003430   \n",
       "3 -0.014864 -0.006269  0.018245 -0.001961  0.000986  0.000688 -0.002638   \n",
       "4  0.001524 -0.002425 -0.008089  0.001256  0.000303  0.003146 -0.009862   \n",
       "\n",
       "       7         8         9       ...         2038      2039      2040  \\\n",
       "0 -0.003171 -0.005278 -0.002190    ...     0.000512 -0.007628  0.007182   \n",
       "1  0.010797  0.002612 -0.003186    ...    -0.002564  0.011355 -0.005319   \n",
       "2 -0.002197 -0.004508  0.011429    ...     0.002081 -0.005939 -0.005204   \n",
       "3 -0.001666 -0.006747 -0.008210    ...    -0.002895  0.010624  0.005879   \n",
       "4 -0.007076  0.011734 -0.001042    ...     0.008517 -0.002372 -0.001015   \n",
       "\n",
       "       2041      2042      2043      2044      2045      2046      2047  \n",
       "0  0.000971  0.013050  0.002880  0.008109 -0.009810  0.004303 -0.001886  \n",
       "1 -0.003187 -0.006499 -0.009484  0.000241  0.015603 -0.003334 -0.003804  \n",
       "2  0.005689  0.002806  0.000785 -0.006149  0.008245  0.014035  0.003263  \n",
       "3 -0.013033  0.004165 -0.000437  0.000303 -0.001383 -0.005297  0.003090  \n",
       "4  0.010934 -0.007737  0.009299 -0.005106 -0.005191 -0.008182 -0.004550  \n",
       "\n",
       "[5 rows x 2048 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit = LogisticRegression(C=1/(2*n*1), fit_intercept=False).fit(x_train, y_train)\n",
    "pd.DataFrame(logit.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.842105\n",
      "Misclassification Error: 13.157895\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: %f' % (accuracy))\n",
    "print('Misclassification Error: %f' % (misclass_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtoAAALrCAYAAADAyCA7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\naOJ0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20d8aabccf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# calculate and display confusion matrix (see function above)\n",
    "conf_mat = display_confusion_matrix(predict=y_predict, actual=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write a function that returns the ranked list of classes in terms of classiﬁcation diﬃculty using the confusion matrix. Compute the _multi-class misclassiﬁcation error_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>misclass error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.0</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class  misclass error\n",
       "0   11.0        0.285714\n",
       "1    2.0        0.250000\n",
       "2   10.0        0.125000\n",
       "3    4.0        0.000000\n",
       "4    1.0        0.000000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see above \"class_diff\" function\n",
    "\n",
    "pd.DataFrame(class_diff(classes, conf_mat), columns=['class', 'misclass error'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Find the values of the regularization parameters $\\lambda_1,\\dots,\\lambda_k$ for the classiﬁers using a hold-out validation set strategy. Deﬁne a grid of values $\\Lambda$ for each parameter $\\lambda_c$ with $c=1,\\ldots,k$. For each setting of the regularization parameters $\\lambda_1,\\ldots,\\lambda_k$, where __each__ $\\lambda_c$ can take values in $\\Lambda$ (independently), train all your $k=5$ classiﬁers and save the _multi-class misclassiﬁcation error_ on the validation set for each setting of the regularization parameters $\\lambda_1,\\ldots,\\lambda_k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# declare lambdas/folds to test cv (NOTE: there are only two lambdas and three folds as this is computationally intensive)\n",
    "lamb_inputs = [0.01, 0.0001]\n",
    "num_folds = 3\n",
    "\n",
    "# create a dictionary of possibilities to be fed into ParameterGrid\n",
    "param_grid = {'class1':lamb_inputs,\n",
    "              'class2':lamb_inputs,\n",
    "              'class4':lamb_inputs,\n",
    "              'class10':lamb_inputs,\n",
    "              'class11':lamb_inputs}\n",
    "\n",
    "# create a grid of all possibilities for the five classes\n",
    "lamb_param_grid = ParameterGrid(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# initialize variable\n",
    "average_misclass_error = []\n",
    "\n",
    "# loop through all possible lambda/class combinations\n",
    "for params in lamb_param_grid:\n",
    "    lamb_list = [params['class1'], params['class2'], params['class4'], params['class10'], params['class11']]\n",
    "    multi_class_misclass_error = []\n",
    "\n",
    "    # loop through folds for cross-validation\n",
    "    for i in range(num_folds):\n",
    "        # resplit data for each fold(1 & 2 & 3)\n",
    "        x_train, x_test, y_train, y_test, test_set = split_data_equal(x_subset, y_subset, test_set, rand_state=i)\n",
    "\n",
    "        # train algorithm and get predictions/multi-class misclassification error\n",
    "        betas = train_alg(x=x_train, y=y_train, classes=classes, lamb_list=lamb_list)\n",
    "        y_predict = predict(x=x_test, betas=betas, classes=classes)\n",
    "        accuracy, misclass_error = accuracy_misclass_error(predict=y_predict, actual=y_test)\n",
    "        multi_class_misclass_error.append(misclass_error)\n",
    "        \n",
    "        # append average multi-class misclassification error\n",
    "        if i == num_folds - 1:\n",
    "            average_misclass_error.append(np.mean(multi_class_misclass_error))\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _This is not scalable!!!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Find the optimal value of the regularization parameters $\\lambda_1,\\ldots,\\lambda_k$ based on the validation error. Display the confusion matrix for this setting of the regularization parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class10_lambda</th>\n",
       "      <th>class11_lambda</th>\n",
       "      <th>class1_lambda</th>\n",
       "      <th>class2_lambda</th>\n",
       "      <th>class4_lambda</th>\n",
       "      <th>misclass_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>7.017544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>7.017544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>7.017544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>7.017544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>7.017544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>7.017544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>7.017544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>7.017544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>9.649123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>9.649123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>9.649123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>9.649123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>9.649123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>9.649123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>9.649123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>9.649123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>9.649123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>9.649123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>9.649123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>9.649123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>9.649123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>9.649123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>9.649123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>9.649123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>12.280702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>12.280702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>12.280702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>12.280702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>12.280702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>12.280702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>12.280702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>12.280702</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    class10_lambda  class11_lambda  class1_lambda  class2_lambda  \\\n",
       "0           0.0001          0.0100         0.0001         0.0001   \n",
       "1           0.0001          0.0100         0.0001         0.0001   \n",
       "2           0.0001          0.0100         0.0001         0.0100   \n",
       "3           0.0001          0.0100         0.0001         0.0100   \n",
       "4           0.0001          0.0100         0.0100         0.0100   \n",
       "5           0.0001          0.0100         0.0100         0.0100   \n",
       "6           0.0001          0.0100         0.0100         0.0001   \n",
       "7           0.0001          0.0100         0.0100         0.0001   \n",
       "8           0.0100          0.0100         0.0100         0.0100   \n",
       "9           0.0001          0.0001         0.0001         0.0100   \n",
       "10          0.0001          0.0001         0.0001         0.0100   \n",
       "11          0.0100          0.0100         0.0001         0.0001   \n",
       "12          0.0100          0.0100         0.0001         0.0001   \n",
       "13          0.0100          0.0100         0.0001         0.0100   \n",
       "14          0.0100          0.0100         0.0001         0.0100   \n",
       "15          0.0001          0.0001         0.0100         0.0001   \n",
       "16          0.0001          0.0001         0.0100         0.0001   \n",
       "17          0.0001          0.0001         0.0100         0.0100   \n",
       "18          0.0001          0.0001         0.0100         0.0100   \n",
       "19          0.0100          0.0100         0.0100         0.0001   \n",
       "20          0.0100          0.0100         0.0100         0.0001   \n",
       "21          0.0100          0.0100         0.0100         0.0100   \n",
       "22          0.0001          0.0001         0.0001         0.0001   \n",
       "23          0.0001          0.0001         0.0001         0.0001   \n",
       "24          0.0100          0.0001         0.0100         0.0001   \n",
       "25          0.0100          0.0001         0.0001         0.0100   \n",
       "26          0.0100          0.0001         0.0001         0.0100   \n",
       "27          0.0100          0.0001         0.0001         0.0001   \n",
       "28          0.0100          0.0001         0.0001         0.0001   \n",
       "29          0.0100          0.0001         0.0100         0.0001   \n",
       "30          0.0100          0.0001         0.0100         0.0100   \n",
       "31          0.0100          0.0001         0.0100         0.0100   \n",
       "\n",
       "    class4_lambda  misclass_error  \n",
       "0          0.0001        7.017544  \n",
       "1          0.0100        7.017544  \n",
       "2          0.0001        7.017544  \n",
       "3          0.0100        7.017544  \n",
       "4          0.0100        7.017544  \n",
       "5          0.0001        7.017544  \n",
       "6          0.0100        7.017544  \n",
       "7          0.0001        7.017544  \n",
       "8          0.0100        9.649123  \n",
       "9          0.0001        9.649123  \n",
       "10         0.0100        9.649123  \n",
       "11         0.0001        9.649123  \n",
       "12         0.0100        9.649123  \n",
       "13         0.0001        9.649123  \n",
       "14         0.0100        9.649123  \n",
       "15         0.0001        9.649123  \n",
       "16         0.0100        9.649123  \n",
       "17         0.0001        9.649123  \n",
       "18         0.0100        9.649123  \n",
       "19         0.0001        9.649123  \n",
       "20         0.0100        9.649123  \n",
       "21         0.0001        9.649123  \n",
       "22         0.0100        9.649123  \n",
       "23         0.0001        9.649123  \n",
       "24         0.0001       12.280702  \n",
       "25         0.0100       12.280702  \n",
       "26         0.0001       12.280702  \n",
       "27         0.0100       12.280702  \n",
       "28         0.0001       12.280702  \n",
       "29         0.0100       12.280702  \n",
       "30         0.0001       12.280702  \n",
       "31         0.0100       12.280702  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize variables\n",
    "class1_lamb = []\n",
    "class2_lamb = []\n",
    "class4_lamb = []\n",
    "class10_lamb = []\n",
    "class11_lamb = []\n",
    "\n",
    "# grab all of the lambda values for each class\n",
    "for i in range(len(lamb_param_grid)):\n",
    "    class1_lamb.append(lamb_param_grid[i]['class1'])\n",
    "    class2_lamb.append(lamb_param_grid[i]['class2'])\n",
    "    class4_lamb.append(lamb_param_grid[i]['class4'])\n",
    "    class10_lamb.append(lamb_param_grid[i]['class10'])\n",
    "    class11_lamb.append(lamb_param_grid[i]['class11'])\n",
    "\n",
    "# put values into a data frame for more readable content\n",
    "df = pd.DataFrame({'class1_lambda':class1_lamb,\n",
    "                   'class2_lambda':class2_lamb,\n",
    "                   'class4_lambda':class4_lamb,\n",
    "                   'class10_lambda':class10_lamb,\n",
    "                   'class11_lambda':class11_lamb,\n",
    "                   'misclass_error':average_misclass_error})\n",
    "\n",
    "# sort by lowest misclassification error to be extracted for confusion matrix\n",
    "df = df.sort_values('misclass_error')\n",
    "df = df.reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare optimal lambdas from my own cross-validation to those from scikit learn's LogisticRegressionCV. This is not a good representative sample as there are only five classes involved and it is currently very accurate at predicting for only five classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0001, 0.01, 0.0001, 0.0001, 0.0001]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lamb_list = np.array(df.iloc[0][0:5]).tolist()\n",
    "lamb_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0001    ,  0.0001    ,  0.00077426,  0.00077426,  0.00599484])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logitCV = LogisticRegressionCV(fit_intercept=False).fit(x_train, y_train)\n",
    "logitCV.C_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.842105\n",
      "Misclassification Error: 13.157895\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtoAAALrCAYAAADAyCA7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\naOJ0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20d87c85ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# reset the data to the original split (for comparison of optimal lambdas)\n",
    "x_train, x_test, y_train, y_test, test_set = split_data_equal(x_subset, y_subset, test_set)\n",
    "\n",
    "# predict using new ideal lambdas from cross-validation and show confusion matrix\n",
    "betas = train_alg(x=x_train, y=y_train, classes=classes, lamb_list=lamb_list)\n",
    "y_predict = predict(x=x_test, betas=betas, classes=classes)\n",
    "accuracy, misclass_error = accuracy_misclass_error(predict=y_predict, actual=y_test)\n",
    "print('Accuracy: %f' % (accuracy))\n",
    "print('Misclassification Error: %f' % (misclass_error))\n",
    "conf_mat = display_confusion_matrix(predict=y_predict, actual=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are not any noticeable differences, however, we are unable to test an appropriate number of different lambdas, and the dataset is rather small when considering only five classes. This is not a scalable approach for a larger dataset or number of lambdas to use for cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
