{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dane Jordan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle Competition - SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import urllib.request\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "x = pickle.load(urllib.request.urlopen('https://s3.amazonaws.com/stat558drjordankaggle/train_features'))\n",
    "test_set = pickle.load(urllib.request.urlopen('https://s3.amazonaws.com/stat558drjordankaggle/test_features'))\n",
    "y = np.array(pickle.load(urllib.request.urlopen('https://s3.amazonaws.com/stat558drjordankaggle/train_labels')))\n",
    "y = y.astype(float)\n",
    "\n",
    "# load transformed image data\n",
    "color_set = pickle.load(urllib.request.urlopen('https://s3.amazonaws.com/stat558drjordankaggle/color_features'))\n",
    "compress_set = pickle.load(urllib.request.urlopen('https://s3.amazonaws.com/stat558drjordankaggle/compress_features'))\n",
    "crop_set = pickle.load(urllib.request.urlopen('https://s3.amazonaws.com/stat558drjordankaggle/crop_features'))\n",
    "crop_to_corner_set = pickle.load(urllib.request.urlopen('https://s3.amazonaws.com/stat558drjordankaggle/crop_to_corner_features'))\n",
    "homography_set = pickle.load(urllib.request.urlopen('https://s3.amazonaws.com/stat558drjordankaggle/homography_features'))\n",
    "mirror_set = pickle.load(urllib.request.urlopen('https://s3.amazonaws.com/stat558drjordankaggle/mirror_features'))\n",
    "rotate30_set = pickle.load(urllib.request.urlopen('https://s3.amazonaws.com/stat558drjordankaggle/rotate30_features'))\n",
    "scale_set = pickle.load(urllib.request.urlopen('https://s3.amazonaws.com/stat558drjordankaggle/scale_features'))\n",
    "\n",
    "# subset the data (if wanting to use a smaller number of classes)\n",
    "classes = np.unique(y)\n",
    "index = np.ravel(np.nonzero(np.in1d(y, classes)))\n",
    "x_subset = x[index]\n",
    "y_subset = y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeobj(beta, lamb, x, y):\n",
    "    # number of observations\n",
    "    n = x.shape[0]\n",
    "    \n",
    "    # compute objective function\n",
    "    obj = (1/n)*np.sum(np.maximum(0, 1 - y*np.dot(x, beta))**2) + lamb*np.sum(beta**2)\n",
    "    \n",
    "    return obj\n",
    "\n",
    "\n",
    "def computegrad(beta, lamb, x, y):\n",
    "    # number of observations\n",
    "    n = x.shape[0]\n",
    "    \n",
    "    # compute gradient of objective function\n",
    "    grad_beta = -(2/n)*np.dot(x.T, y*np.maximum(0, 1 - y*np.dot(x, beta))) + 2*lamb*beta\n",
    "    \n",
    "    return grad_beta\n",
    "\n",
    "\n",
    "def backtracking(beta, lamb, x, y, eta=1, alpha=0.5, gamma=0.8, max_iter=100):\n",
    "    # initialize variables\n",
    "    grad_beta = computegrad(beta, lamb, x, y)\n",
    "    norm_grad_beta = np.sqrt(np.sum(grad_beta**2))\n",
    "    found_eta = 0\n",
    "    t = 0\n",
    "    \n",
    "    # loop through until eta found or max iterations reached\n",
    "    while found_eta == 0 and t < max_iter:\n",
    "        if (computeobj(beta - eta*grad_beta, lamb, x, y) <\n",
    "                computeobj(beta, lamb, x, y) - alpha*eta*norm_grad_beta**2):\n",
    "            found_eta = 1\n",
    "        elif t == max_iter:\n",
    "            break\n",
    "        else:\n",
    "            eta = eta*gamma\n",
    "            t += 1\n",
    "    \n",
    "    return eta\n",
    "\n",
    "\n",
    "def mylinearsvm(beta_init, theta_init, lamb, x, y, max_iter, eps):\n",
    "    # initialize variables\n",
    "    beta = beta_init\n",
    "    theta = theta_init\n",
    "    grad_theta = computegrad(theta, lamb, x, y)\n",
    "    eta_init = 1/(max(np.linalg.eigh(np.dot((1/n)*x.T, x))[0]) + lamb)\n",
    "    beta_vals = [beta_init]\n",
    "    t = 0\n",
    "    \n",
    "    # loop through until EITHER max iterations reached or threshold of epsilon reached\n",
    "    while t < max_iter and np.linalg.norm(grad_theta) >  eps:\n",
    "        eta = backtracking(beta, lamb, x, y, eta=eta_init)\n",
    "        beta_next = theta - eta*grad_theta\n",
    "        theta = beta_next + t*(beta_next - beta)/(t + 3)\n",
    "        grad_theta = computegrad(theta, lamb, x, y)\n",
    "        beta = beta_next\n",
    "        beta_vals.append(beta)\n",
    "        t += 1\n",
    "        \n",
    "    return beta_vals\n",
    "\n",
    "\n",
    "def split_data_equal(x, y, test_set, train_size=0.75):\n",
    "    # split into train and test sets\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=train_size, random_state=0, stratify=y)\n",
    "    \n",
    "    # center and standardize x values\n",
    "    x_scaler = StandardScaler().fit(x_train)\n",
    "    x_train = x_scaler.transform(x_train)\n",
    "    x_test = x_scaler.transform(x_test)\n",
    "    test_set = x_scaler.transform(test_set)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test, test_set\n",
    "\n",
    "\n",
    "def train_alg(x, y, classes, lamb_list):\n",
    "    # initialize betas and create list for final beta values\n",
    "    d = x_train.shape[1]\n",
    "    beta_init = np.zeros(d)\n",
    "    theta_init = np.zeros(d)\n",
    "    final_betas = []\n",
    "    y_values = []\n",
    "\n",
    "    # loop through each label and perform ovr appending the final betas for each class\n",
    "    for i in range(len(classes)):\n",
    "        lamb = lamb_list[i]\n",
    "        y_binary = copy.deepcopy(y)\n",
    "        y_binary[y != classes[i]] = -1\n",
    "        y_binary[y == classes[i]] = 1\n",
    "        betas = mylinearsvm(beta_init=beta_init, theta_init=theta_init, lamb=lamb, x=x_train, y=y_binary, max_iter=1000, eps=1e-5)[-1]\n",
    "        final_betas.append(betas)\n",
    "        \n",
    "    return np.array(final_betas)\n",
    "\n",
    "\n",
    "def predict(x, betas, classes):\n",
    "    # initialize calculated y values\n",
    "    y_values = []\n",
    "    \n",
    "    # loop through set of final betas and calculate y values\n",
    "    for i in range(len(betas)):\n",
    "        y_values.append(np.dot(x, betas[i]))\n",
    "    \n",
    "    # calculate predicted values\n",
    "    y_predict = classes[np.argmax(np.array(y_values), axis=0)]\n",
    "    \n",
    "    return y_predict\n",
    "\n",
    "\n",
    "def accuracy_misclass_error(predict, actual):\n",
    "    # calculate misclassification error\n",
    "    misclass_error = np.mean(predict != actual)*100\n",
    "    accuracy = 100 - misclass_error\n",
    "    \n",
    "    return accuracy, misclass_error\n",
    "\n",
    "\n",
    "def display_confusion_matrix(predict, actual):\n",
    "    # calculate confusion matrix\n",
    "    conf_mat = confusion_matrix(y_true=actual, y_pred=predict)\n",
    "    \n",
    "    # build visual plot\n",
    "    plt.matshow(conf_mat);\n",
    "    plt.title('Confusion Matrix');\n",
    "    plt.xlabel('Predicted Label');\n",
    "    plt.ylabel('True Label');\n",
    "    plt.xticks(range(len(classes)), classes);\n",
    "    plt.yticks(range(len(classes)), classes);\n",
    "    \n",
    "    fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "    fig_size[0] = 18\n",
    "    fig_size[1] = 12\n",
    "    plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "    \n",
    "    return conf_mat\n",
    "    \n",
    "\n",
    "def class_diff(classes, conf_matrix):\n",
    "    # initialize variable to append each individual class percent\n",
    "    percent_correct = []\n",
    "    \n",
    "    # loop through confusion matrix by true label\n",
    "    for i in range(len(conf_matrix[0, :])):\n",
    "        class_count = np.sum(conf_matrix[i])\n",
    "        misclass_count = 0\n",
    "        \n",
    "        # loop through confusion matrix by predict label and append percent correct\n",
    "        for j in range(len(conf_matrix[:, 0])):\n",
    "            if i != j:\n",
    "                misclass_count += conf_matrix[i][j]\n",
    "            else:\n",
    "                pass\n",
    "        percent_correct.append(misclass_count/class_count)\n",
    "        \n",
    "    # calcuate ordered list of multi-class misclassification error\n",
    "    ordered_class_diff = np.vstack((classes, np.array(percent_correct))).T\n",
    "    ordered_class_diff = ordered_class_diff[ordered_class_diff[:, 1].argsort()[::-1]]\n",
    "    \n",
    "    return ordered_class_diff\n",
    "\n",
    "\n",
    "def export_csv(y, name):\n",
    "    y = y.astype(int)\n",
    "    df = pd.DataFrame({'Id':np.arange(1, y.shape[0] + 1), 'Prediction':y})\n",
    "    df.to_csv(name, sep=',', index=False)\n",
    "    \n",
    "\n",
    "def decomp_PCA(train, test, test_set, explained_var_threshold=0.95):\n",
    "    pca = PCA().fit(train)\n",
    "\n",
    "    pca_explained_var_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "    pca_explained_var = []\n",
    "    num_component_vectors = 0\n",
    "\n",
    "    while np.sum(pca_explained_var) < explained_var_threshold:\n",
    "        pca_explained_var.append(pca_explained_var_ratio[num_component_vectors])\n",
    "        num_component_vectors += 1\n",
    "    #print('# Component Vectors: %d    Explained Var: %f' % (num_component_vectors, np.sum(pca_explained_var)))\n",
    "\n",
    "    pca = PCA(n_components=num_component_vectors).fit(train)\n",
    "    x_train = pca.transform(train)\n",
    "    x_test = pca.transform(test)\n",
    "    test_set = pca.transform(test_set)\n",
    "    \n",
    "    return x_train, x_test, test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running sklearn's SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# try stacking image tansformations onto the original features\n",
    "allfeatures = np.vstack((x))\n",
    "alllabels = np.hstack((y))\n",
    "test_set = pickle.load(urllib.request.urlopen('https://s3.amazonaws.com/stat558drjordankaggle/test_features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split data\n",
    "x_train, x_test, y_train, y_test, test_set = split_data_equal(x=allfeatures, y=alllabels, test_set=test_set, train_size=.75)\n",
    "n = x_train.shape[0]\n",
    "d = x_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run PCA to reduce dimensionality\n",
    "x_train, x_test, test_set = decomp_PCA(train=x_train, test=x_test, test_set=test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFitting various models is time intensive, especially when trying out multiple sets of features.\\nFor this reason, the code below has been commented out and the allfeatures and alllabels are\\nset statically at what they were last processed as. Pickle files were used to save the models\\nand are called in the last cell to show results.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Fitting various models is time intensive, especially when trying out multiple sets of features.\n",
    "For this reason, the code below has been commented out and the allfeatures and alllabels are\n",
    "set statically at what they were last processed as. Pickle files were used to save the models\n",
    "and are called in the last cell to show results.\n",
    "'''\n",
    "\n",
    "# rbf kernel ovo\n",
    "# rbfSVC = SVC()\n",
    "# parameters = {'C':[10**i for i in range(-2, 2)]}\n",
    "# rbfCV = GridSearchCV(rbfSVC, parameters, n_jobs=-1).fit(x_train, y_train)\n",
    "\n",
    "# polynomial (order 2) kernel ovo\n",
    "# poly2SVC = SVC(kernel='poly', degree=2)\n",
    "# parameters = {'C':[10**i for i in range(-2, 2)]}\n",
    "# poly2CV = GridSearchCV(poly2SVC, parameters, n_jobs=-1).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# y_predict_rbf = rbfCV.predict(x_test)\n",
    "# y_predict_poly2 = poly2CV.predict(x_test)\n",
    "# print('Accuracy: %f%%' % (np.mean(y_predict_rbf == y_test)*100))\n",
    "# print('Accuracy: %f%%' % (np.mean(y_predict_poly2 == y_test)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for purposes of efficiency, the models were saved using pickle\n",
    "all_test_set_predictions_header = ['Id']\n",
    "all_test_set_predictions = np.arange(1, 4321)\n",
    "\n",
    "# declare a dictionary for models\n",
    "my_dict = {}\n",
    "\n",
    "# image transformations feature sets\n",
    "feature_sets = ['base',\n",
    "                'color',\n",
    "                'compress',\n",
    "                'crop',\n",
    "                'crop_to_corner',\n",
    "                'homography',\n",
    "                'mirror',\n",
    "                'rotate30',\n",
    "                'scale']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rbf kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rbf\n",
      "adding image transformations 1 at a time\n",
      "base : 58.055556%\n",
      "color : 52.037037%\n",
      "compress : 44.444444%\n",
      "crop : 72.453704%\n",
      "crop_to_corner : 51.296296%\n",
      "homography : 84.259259%\n",
      "mirror : 87.129630%\n",
      "rotate30 : 37.314815%\n",
      "scale : 51.805556%\n"
     ]
    }
   ],
   "source": [
    "# rbf - print all past results for each image transformation added to the base set\n",
    "print('rbf\\nadding image transformations 1 at a time')\n",
    "for i in feature_sets:\n",
    "    my_dict['rbf_SVCCV_{0}'.format(i)] = pickle.load(urllib.request.urlopen('https://s3.amazonaws.com/stat558drjordankaggle/SVC/rbf_SVCCV_' + i))\n",
    "    if i == 'base':\n",
    "        allfeatures = x\n",
    "        alllabels = y\n",
    "    else:\n",
    "        allfeatures = np.vstack((x, globals()[i + '_set']))\n",
    "        alllabels = np.hstack((y, y))\n",
    "    test_set = pickle.load(urllib.request.urlopen('https://s3.amazonaws.com/stat558drjordankaggle/test_features'))\n",
    "    x_train, x_test, y_train, y_test, test_set = split_data_equal(x=allfeatures, y=alllabels, test_set=test_set, train_size=.75)\n",
    "    x_train, x_test, test_set = decomp_PCA(train=x_train, test=x_test, test_set=test_set)\n",
    "    all_test_set_predictions_header = all_test_set_predictions_header + ['rbf_' + i]\n",
    "    all_test_set_predictions = np.vstack((all_test_set_predictions, my_dict['rbf_SVCCV_' + i].predict(test_set)))\n",
    "    print(i, ': %f%%' % (np.mean(my_dict['rbf_SVCCV_' + i].predict(x_test) == y_test)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### polynomial (order 2) kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polynomial(order 2)\n",
      "adding image transformations 1 at a time\n",
      "base : 67.037037%\n",
      "color : 60.925926%\n",
      "compress : 51.805556%\n",
      "crop : 83.750000%\n",
      "crop_to_corner : 59.629630%\n",
      "homography : 90.925926%\n",
      "mirror : 92.685185%\n",
      "rotate30 : 43.379630%\n",
      "scale : 62.361111%\n"
     ]
    }
   ],
   "source": [
    "# polynomial (order 2) - print all past results for each image transformation added to the base set\n",
    "print('polynomial(order 2)\\nadding image transformations 1 at a time')\n",
    "for i in feature_sets:\n",
    "    my_dict['poly2_SVCCV_{0}'.format(i)] = pickle.load(urllib.request.urlopen('https://s3.amazonaws.com/stat558drjordankaggle/SVC/poly2_SVCCV_' + i))\n",
    "    if i == 'base':\n",
    "        allfeatures = x\n",
    "        alllabels = y\n",
    "    else:\n",
    "        allfeatures = np.vstack((x, globals()[i + '_set']))\n",
    "        alllabels = np.hstack((y, y))\n",
    "    test_set = pickle.load(urllib.request.urlopen('https://s3.amazonaws.com/stat558drjordankaggle/test_features'))\n",
    "    x_train, x_test, y_train, y_test, test_set = split_data_equal(x=allfeatures, y=alllabels, test_set=test_set, train_size=.75)\n",
    "    x_train, x_test, test_set = decomp_PCA(train=x_train, test=x_test, test_set=test_set)\n",
    "    all_test_set_predictions_header = all_test_set_predictions_header + ['poly2_' + i]\n",
    "    all_test_set_predictions = np.vstack((all_test_set_predictions, my_dict['rbf_SVCCV_' + i].predict(test_set)))\n",
    "    print(i, ': %f%%' % (np.mean(my_dict['poly2_SVCCV_' + i].predict(x_test) == y_test)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### output all predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_test_set_predictions.astype(int).T)\n",
    "df.to_csv('All_SVC_Predictions.csv', sep=',', header=all_test_set_predictions_header, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
